{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Write a function word_freq() that takes a word and the name of a section of the Brown Corpus as arguments, and computes the frequency of the word in that section of the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq(word, sec):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        word (str): word for which frequency count is to be calculated.\n",
    "        sec (str): a category of brown corpus.\n",
    "    Returns:\n",
    "        num (int): frequency of `word` in `sec` section of brown corpus.\n",
    "    \"\"\"\n",
    "    freq_dist = FreqDist(brown.words(categories=sec))\n",
    "    num = freq_dist[word]\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq('India', 'news')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Write a program to guess the number of syllables contained in a text, making use of the CMU Pronouncing Dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syllables(txt):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        txt (list): A list of tokens\n",
    "    Returns:\n",
    "        syll_num (int): Return the number of syllables in `txt`.\n",
    "    \"\"\"\n",
    "    cmud = nltk.corpus.cmudict.dict()\n",
    "    syll_text = []\n",
    "    for word in txt:\n",
    "        syll_text.extend(cmud[word.lower()][0])\n",
    "    syll_num = len(syll_text)\n",
    "    return syll_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "txt = [\"Luke\", \"I'm\", \"your\", \"father\"]\n",
    "print(get_syllables(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Write a function that finds the 50 most frequently occurring words of a text that are not stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_freq_non_stopwords(text, n=50):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        text (list): A list of tokens\n",
    "    Returns:\n",
    "        int (list): Return most frequent non-stopwords in `text`.\n",
    "    \"\"\"\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    allWordExceptStopDist = nltk.FreqDist(w.lower() for w in text if w not in stopwords)\n",
    "    most_freq = allWordExceptStopDist.most_common(n)\n",
    "    return [x for (x, y) in most_freq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'the', '``', \"''\", 'said', ';', '--', 'mrs.', 'would', 'new', 'one', 'he', 'i', 'last', 'two', ')', 'mr.', '(', 'first', 'state', ':', 'year', 'president', 'a', 'home', 'also', 'in', 'it', 'but', 'made', 'time', 'years', 'three', 'house', 'week', 'city', 'may', '?', 'school', 'could', 'four', 'day', 'committee', 'man', 'members', 'back', 'government', 'many', 'national']\n"
     ]
    }
   ],
   "source": [
    "print(most_freq_non_stopwords(brown.words(categories='news')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Write a Python NLTK program to compare the similarity of two given verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def calc_similarity(verb1, verb2):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        verb1 (str): verb 1\n",
    "        verb2 (str): verb 2\n",
    "    Returns:\n",
    "        sim (float): The extent of similarity between the two words.\n",
    "    \"\"\"\n",
    "    v1 = wordnet.synset(verb1)\n",
    "    v2 = wordnet.synset(verb2)\n",
    "    sim = v1.wup_similarity(v2)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "print(calc_similarity('sprint.v.01', 'run.v.01'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Define a function find_language() that takes a string as its argument, and returns a list of languages that have that string as a word. Use the udhr corpus and limit your searches to files in the Latin-1 encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr\n",
    "\n",
    "def find_language(string):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        string (str): a word in any language\n",
    "    Returns:\n",
    "        target_lang (list): List of languages in which the `string` appears.\n",
    "    \"\"\"\n",
    "    languages = [name for name in udhr.fileids() if 'Latin1' in name]\n",
    "    target_lang = [lang for lang in languages if string in udhr.words(lang)]\n",
    "    return target_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['French_Francais-Latin1']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_language('homme')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. The polysemy of a word is the number of senses it has. Using WordNet, we can determine that the noun dog has 7 senses with: len(wn.synsets('dog', 'n')). Compute the average polysemy of nouns, verbs, adjectives and adverbs according to WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_polysemy(groups):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        groups (dict): a dict of wordnet groups and respective object.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    all_synsets_len = len([synst for synst in wordnet.all_synsets()])\n",
    "    \n",
    "    for (k, v) in groups.items():\n",
    "        synset_len = len([synst for synst in wordnet.all_synsets(pos=v)])\n",
    "        print('{0}\\t::\\t{1}'.format(k, all_synsets_len/synset_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb\t::\t8.546451659766108\n",
      "Noun\t::\t1.4328563599829507\n",
      "Adj\t::\t6.480447235073805\n",
      "Adv\t::\t32.49351008008837\n"
     ]
    }
   ],
   "source": [
    "groups = {'Verb': wordnet.VERB, 'Noun': wordnet.NOUN,\n",
    "          'Adj': wordnet.ADJ, 'Adv': wordnet.ADV}\n",
    "\n",
    "avg_polysemy(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Define a function hedge(text) which processes a text and produces a new version with the word 'like' between every third word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hedge(text_list):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        text_list (str): list of strings\n",
    "    Returns:\n",
    "        text_list (str): list of strings with like inserted after every 3rd \n",
    "    \"\"\"\n",
    "    \n",
    "    total = len(text_list)//4\n",
    "    counter = 1\n",
    "    for i in range(total):\n",
    "        text_list.insert(((i+1)*4 - counter), 'LIKES')\n",
    "\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Captain', 'Hans', 'Solo', 'LIKES', 'Princess', 'Leia', 'Organa', 'LIKES', 'Master', 'Luke', 'Skywalker']\n"
     ]
    }
   ],
   "source": [
    "text = ['Captain', 'Hans', 'Solo', 'Princess', 'Leia', 'Organa', 'Master', 'Luke', 'Skywalker']\n",
    "\n",
    "print(hedge(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "defaultvenv",
   "language": "python",
   "name": "defaultvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
